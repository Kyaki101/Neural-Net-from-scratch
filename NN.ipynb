{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aoOGtVux6oe"
      },
      "source": [
        "# Trabajo Práctico 3\n",
        "\n",
        "Emmanuel Rojas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BesyaS9kQ4ct"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles, make_moons, make_blobs\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rixl0UpYMB8J"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6TpKfWYRGhS"
      },
      "source": [
        "Neural net from scratch using only torch tensors for gpu acceleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibKKFZsYRNp7"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This class implements a simple multi-layer feedforward neural network from scratch using PyTorch.\n",
        "\n",
        "It is designed to be a flexible, manually-coded neural network where the user can specify the\n",
        "architecture (number of neurons per layer). The network uses the sigmoid activation function\n",
        "for both the hidden and output layers.\n",
        "\"\"\"\n",
        "class Neural_Network:\n",
        "    \"\"\"\n",
        "    Initializes the neural network with the number of neurons per layer, the learning rate and the maximum weight.\n",
        "    \"\"\"\n",
        "    def __init__(self, neurons_per_layer, alpha, max_weights):\n",
        "        self.neurons_per_layer = neurons_per_layer\n",
        "        self.alpha = alpha\n",
        "        self.max_weights = max_weights\n",
        "\n",
        "        #Initializes the weight matrix (W_0) between the input and hidden layer\n",
        "        #An extra row/column is added to handle the bias\n",
        "        self.W_0 = torch.rand((self.neurons_per_layer[1] + 1, self.neurons_per_layer[0] + 1), device=device)\n",
        "\n",
        "        #The first row of weight matrix is set to NaN\n",
        "        self.W_0[0] = torch.full_like(self.W_0[0], torch.nan)\n",
        "\n",
        "        #Initializes the weight matrix (W_1)  between the hidden and output layer\n",
        "        self.W_1 = torch.rand((self.neurons_per_layer[2], self.neurons_per_layer[1] + 1), device=device)\n",
        "\n",
        "    \"\"\"\n",
        "    Performs the forward pass of the network.\n",
        "    It takes an input tensor X and returns the output of the network.\n",
        "    \"\"\"\n",
        "    def forward(self, X):\n",
        "        if(len(X) != self.neurons_per_layer[0]):\n",
        "            raise Exception(\"The input layer doesn't match original size\")\n",
        "\n",
        "        #Adds a bias term (row of ones) to the input matrix X\n",
        "        bias = torch.ones((1, len(X[0])), device=device)\n",
        "        X = torch.vstack((bias, X))\n",
        "\n",
        "        #Computes the weighted sum for the hidden layer Y\n",
        "        self.Y = self.W_0 @ X\n",
        "\n",
        "        #Applies the sigmoid activation function to the hidden layer\n",
        "        self.Y_a = torch.sigmoid(self.Y)\n",
        "\n",
        "        #Sets the first row of the activated hidden layer to 1, representing the bias for the next layer.\n",
        "        self.Y_a[0] = torch.ones_like(self.Y_a[0])\n",
        "\n",
        "        #Computes the weighted sum for the output layer Z\n",
        "        self.Z = self.W_1 @ self.Y_a\n",
        "\n",
        "        #Applies the sigmoid activation function to get the final output (Z_a)\n",
        "        self.Z_a = torch.sigmoid(self.Z)\n",
        "\n",
        "        #Returns the predicted class and the output probabilities\n",
        "        return torch.argmax(self.Z_a), self.Z_a\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates the Mean Squared Error (MSE) loss.\n",
        "    This measures how different the network's prediction (self.Z_a) is from the true target (t).\n",
        "    \"\"\"\n",
        "    def calc_loss(self, t):\n",
        "        return torch.mean((self.Z_a - t) ** 2)\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates the accuracy of the model's predictions.\n",
        "    It compares the predicted class indices with the true class indices.\n",
        "    \"\"\"\n",
        "    def calc_accuracy(self, x, t):\n",
        "        #Compares the index of the max value for predictions and targets\n",
        "        res = torch.sum(x.argmax(dim=0) == t.argmax(dim=0), dim=0)\n",
        "        return res / len(x[0]) #returns the mean accuracy\n",
        "\n",
        "    def f1(self, x, t):\n",
        "        tp = torch.sum(x.argmax(dim=0) == t.argmax(dim=0), dim=0)\n",
        "        tn = torch.sum(x.argmin(dim=0) == t.argmin(dim=0), dim=0)\n",
        "        fp = torch.sum(x.argmax(dim=0) != t.argmax(dim=0), dim=0)\n",
        "        fn = torch.sum(x.argmin(dim=0) != t.argmin(dim=0), dim=0)\n",
        "        precision = tp / (tp + fp)\n",
        "        recall = tp / (tp + fn)\n",
        "        return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    def calc_f1(self, x, t):\n",
        "        self.forward(x)\n",
        "        return self.f1(self.Z_a, t)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates the derivative of the MSE loss function with respect to the output activations.\n",
        "    This is the first step in backpropagation.\n",
        "    \"\"\"\n",
        "    def loss_deriv(self, t):\n",
        "        return 2/self.Z_a.numel() * (self.Z_a - t)\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates the derivative of the sigmoid activation function.\n",
        "    \"\"\"\n",
        "    def sigmoid_dx(self, activated):\n",
        "        return activated * (1 - activated)\n",
        "\n",
        "    \"\"\"\n",
        "    Performs the backpropagation algorithm.\n",
        "    It calculates the gradients of the loss with respect to each weight matrix (W_0 and W_1).\n",
        "    These gradients indicate how much each weight should change to reduce the loss.\n",
        "    \"\"\"\n",
        "    def backwards(self, X, t):\n",
        "        bias = torch.ones((1, len(X[0])), device=device)\n",
        "        X = torch.vstack((bias, X))\n",
        "\n",
        "        #  Gradient of Loss w.r.t. Output (dL/dO)\n",
        "        dL_dO = self.loss_deriv(t)\n",
        "\n",
        "        # Gradient of Output w.r.t. pre-activation Output (dO/dZ)\n",
        "        dO_dZ = self.sigmoid_dx(self.Z_a)\n",
        "\n",
        "        # Gradient of Loss w.r.t. pre-activation Output (dL/dZ)\n",
        "        dL_dZ = dL_dO * dO_dZ\n",
        "\n",
        "        # Gradient of Loss w.r.t. weights W1 (dL/dW1)\n",
        "        # This is the gradient for the hidden-to-output layer weights.\n",
        "        dL_dW1 = dL_dZ @ self.Y_a.T #First grad\n",
        "\n",
        "        # Propagate the error back to the hidden layer's activations (dL/dYa)\n",
        "        dL_dYa = self.W_1.T @ dL_dZ\n",
        "\n",
        "        # Gradient of Loss w.r.t. pre-activation Hidden layer (dL/dY)\n",
        "        dL_dY = self.sigmoid_dx(self.Y_a) * dL_dYa\n",
        "\n",
        "        # This is the gradient for the input-to-hidden layer weights.\n",
        "        dL_dW0 = dL_dY @ X.T #Second grad\n",
        "\n",
        "        return (dL_dW0, dL_dW1)\n",
        "\n",
        "\n",
        "    # Updates the network´s weights using gradient descent\n",
        "    def optimize(self, alpha):\n",
        "        # Update weights by moving in the opposite direction of the gradient.\n",
        "        self.W_0 = self.W_0 - alpha * self.del_W0\n",
        "        self.W_1 = self.W_1 - alpha * self.del_W1\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    The main training loop of the neural network.\n",
        "    It iterates for a specified number of epochs, performing forward pass, backpropagation,\n",
        "    and weight updates. It also evaluates the model on validation data.\n",
        "    \"\"\"\n",
        "    def train_nn(self, X_train, T_train, X_valid, T_valid, epochs = 50, alpha = 0.1, gamma = 0.1, plot = False):\n",
        "\n",
        "        epoch_count = []\n",
        "        loss_count = []\n",
        "        accuracy_count = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            #Perform forward pass\n",
        "            self.forward(X_train)\n",
        "\n",
        "            #Perform backprop\n",
        "            self.del_W0, self.del_W1 = self.backwards(X_train, T_train)\n",
        "\n",
        "            self.optimize(alpha)\n",
        "\n",
        "            #Check loss against validation\n",
        "\n",
        "            self.forward(X_valid)\n",
        "            valid_loss = self.calc_loss(T_valid)\n",
        "            valid_acc = self.calc_accuracy(self.Z_a, T_valid)\n",
        "\n",
        "            # Store metrics for plotting.\n",
        "            epoch_count.append(epoch)\n",
        "            loss_count.append(valid_loss)\n",
        "            accuracy_count.append(valid_acc)\n",
        "\n",
        "        # If plot is True, visualize the training progress.\n",
        "        if(plot):\n",
        "            epoch_count = torch.tensor(epoch_count, device=\"cpu\")\n",
        "            loss_count = torch.tensor(loss_count, device=\"cpu\")\n",
        "            accuracy_count = torch.tensor(accuracy_count, device=\"cpu\")\n",
        "            plt.plot(epoch_count, loss_count)\n",
        "            plt.xlabel(\"Épocas\")\n",
        "            plt.ylabel(\"Pérdida\")\n",
        "            plt.show()\n",
        "            plt.plot(epoch_count, accuracy_count)\n",
        "            plt.xlabel(\"Épocas\")\n",
        "            plt.ylabel(\"Precisión\")\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uw3vO46BY-t"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor([\n",
        "    [1, 1, 0, 0],\n",
        "    [1, 0, 1, 0]\n",
        "], dtype=torch.float, device=device)\n",
        "T_train = torch.tensor([\n",
        "    [1, 0, 0, 1],\n",
        "    [0, 1, 1, 0]\n",
        "], device=device)\n",
        "\n",
        "nn = Neural_Network((2, 2, 2), 0.1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "ejWcZ3OeCsek",
        "outputId": "f9963d81-601a-401d-eb95-3c6bd90523ab"
      },
      "outputs": [],
      "source": [
        "nn.train_nn(X_train=X_train, T_train=T_train, X_valid=X_train, T_valid=T_train, epochs=10000, alpha = 0.8, plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLlWUc9Qh9OA",
        "outputId": "36cc0721-1b7a-44fd-f1b5-a0f95c08f100"
      },
      "outputs": [],
      "source": [
        "X_test = torch.tensor([\n",
        "    [0],\n",
        "    [1]\n",
        "], device = device)\n",
        "\n",
        "res, out = nn.forward(X_test)\n",
        "print(res, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XqDnPqSVwjw"
      },
      "source": [
        "# Clasificación en $\\mathbb{R}^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "gwiBBzedhjuQ",
        "outputId": "ebb59392-7aa3-4ade-e999-ede270b119f3"
      },
      "outputs": [],
      "source": [
        "def gen_data(data_dist, plot=False):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates and preprocesses a dataset for binary classification.\n",
        "    This function can create either blob-like or moon-shaped data clusters using\n",
        "    scikit-learn's dataset generators. It then converts the data into PyTorch tensors,\n",
        "    applies one-hot encoding to the labels, transposes the matrices to fit the\n",
        "    neural network's expected input format (features x samples), and finally splits\n",
        "    the data into training and validation sets.\n",
        "    \"\"\"\n",
        "\n",
        "    # Default to generating blob-like data using scikit-learn's make_blobs\n",
        "    X, y = make_blobs(centers=2, cluster_std=1.5, random_state=1) #ignore error\n",
        "\n",
        "    # If \"moons\" is specified, overwrite the blob data with moon-shaped data\n",
        "    if(data_dist == \"moons\"):\n",
        "        X, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "\n",
        "    # If \"moons\" is specified, overwrite the blob data with moon-shaped data\n",
        "    if(plot):\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.show()\n",
        "\n",
        "    # Convert the NumPy arrays (X, y) to PyTorch tensors and move them to the selected device\n",
        "    X = torch.tensor(X, device=device, dtype=torch.float)\n",
        "    y = torch.tensor(y, device=device)\n",
        "\n",
        "    # Create a zero tensor of shape (number of samples, number of classes)\n",
        "    one_hot = torch.zeros((len(y), 2), device=device)\n",
        "\n",
        "    # Create a zero tensor of shape (number of samples, number of classes)\n",
        "    one_hot[torch.arange(y.size(0)), y] = 1\n",
        "\n",
        "    # Transpose the one-hot tensor to have the shape (number of classes, number of samples)\n",
        "    y = one_hot.T\n",
        "\n",
        "    # Transpose the input data X to match the expected network format (features, samples)\n",
        "    X = X.T\n",
        "\n",
        "    # Calculate the split point for an 80/20 train-validation split\n",
        "    split = int(len(X[0]) *  0.8)\n",
        "\n",
        "    # Split the data into training (first 80%) and validation (last 20%) sets\n",
        "    X_train = X[:, :split]\n",
        "    X_val = X[:, split:]\n",
        "\n",
        "    y_train = y[:, :split]\n",
        "    y_val = y[:, split:]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val\n",
        "\n",
        "X_train, y_train, X_val, y_val = gen_data(\"blobs\", plot=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "QQMBJ80pWRb3",
        "outputId": "35fa710d-3fe0-483a-b965-b622e45f8d3c"
      },
      "outputs": [],
      "source": [
        "r2net = Neural_Network((2, 4, 2), 0.1, 1)\n",
        "r2net.train_nn(X_train=X_train, T_train=y_train, X_valid=X_val, T_valid=y_val, alpha=0.7, plot=True, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "FafbNiUlNoWo",
        "outputId": "bc7b9076-309f-4c0b-8f41-ce7634f3277a"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_val, y_val = gen_data(\"moons\", plot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "id": "5caV-iRjPfap",
        "outputId": "7a536e42-2348-41a6-9eb4-c571258db6fa"
      },
      "outputs": [],
      "source": [
        "r3net = Neural_Network((2, 4, 2), 0.1, 1)\n",
        "r3net.train_nn(X_train=X_train, T_train=y_train, X_valid=X_val, T_valid=y_val, alpha=0.7, plot=True, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PizVJBojjqPc"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "with open(\"SMSSpamCollection\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        parts = line.strip().split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            d1, d2 = parts\n",
        "            data.append([d1, d2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "3bHoxofGlwGg",
        "outputId": "2490d469-f9b1-49b5-b1af-33f41b3f5e1a"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnH2XBVkmraB"
      },
      "outputs": [],
      "source": [
        "def get_bert_embedding(text):\n",
        "    \"\"\"\n",
        "    Generates a sentence-level embedding for a given text using a pre-trained BERT model.\n",
        "\n",
        "    This function takes a string as input, tokenizes it to be BERT-compatible,\n",
        "    and then passes it through the model to get a fixed-size numerical representation.\n",
        "    It specifically uses the model's \"pooler output\" as the final embedding.\n",
        "    \"\"\"\n",
        "    # Convert the raw text into numerical tokens that the BERT model can process.\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # This is crucial for inference as it reduces memory consumption and speeds up computation,\n",
        "    # since we don't need to track operations for backpropagation.\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Pass the tokenized inputs to the pre-trained model.\n",
        "    return outputs.pooler_output.squeeze().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFAgIM_4odon"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "inputs = torch.tensor([])\n",
        "for i in data:\n",
        "    current = torch.tensor(get_bert_embedding(i[1]))\n",
        "    current = current.unsqueeze(1)\n",
        "    if(len(inputs) == 0):\n",
        "        inputs = current\n",
        "    else:\n",
        "        inputs = torch.hstack((inputs, current))\n",
        "    if(i[0] == \"ham\"):\n",
        "        labels.append(1)\n",
        "    else: labels.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg_mNzdGp4bM",
        "outputId": "9d126a6a-811c-4040-8324-1c423f158dea"
      },
      "outputs": [],
      "source": [
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-1wK1uh1LC2",
        "outputId": "6b215de6-fd62-4f6c-96fe-f62507ffd3ec"
      },
      "outputs": [],
      "source": [
        "labels = torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg3mwDO7100C"
      },
      "outputs": [],
      "source": [
        "def split_data(X, t):\n",
        "    \"\"\"\n",
        "    Preprocesses and splits feature and target data into training, validation, and test sets.\n",
        "\n",
        "    This function first normalizes the input feature matrix `X` and then splits both\n",
        "    `X` and the target labels `t` into a 70% training set, a 15% validation set,\n",
        "    and a 15% test set. The target labels `t` are converted into a one-hot\n",
        "    encoded format before being split.\n",
        "    \"\"\"\n",
        "    # Normalize the feature tensor X.\n",
        "    # Scale the result by 100. This step amplifies the magnitude of the feature vectors after they have been scaled down by normalization.\n",
        "    X = torch.nn.functional.normalize(X, dim=1) * 100\n",
        "\n",
        "    # Determine the number of samples for the training set (70% of the data).\n",
        "    trainer = int(len(X[0]) * 0.7)\n",
        "\n",
        "    # Determine the number of samples for the validation set (15% of the data).\n",
        "    valider = int(len(X[0]) * 0.15)\n",
        "\n",
        "    # The training set contains the first 70% of the samples.\n",
        "    X_train = X[:, : trainer]\n",
        "    # The validation set contains the next 15% of the samples.\n",
        "    X_val = X[:, trainer : trainer + valider]\n",
        "    # The test set contains the remaining 15% of the samples.\n",
        "    X_test = X[:, trainer + valider :]\n",
        "\n",
        "    # Create a zero tensor with shape [number_of_classes, number_of_samples].\n",
        "    one_hot = torch.zeros((2, len(X[0])))\n",
        "\n",
        "    # Perform one-hot encoding in a single, efficient operation.\n",
        "    one_hot[t, torch.arange(len(X[0]))] = 1\n",
        "\n",
        "    # Split the one-hot encoded labels to correspond with the feature splits.\n",
        "    y_train = one_hot[:, : trainer]\n",
        "    y_val = one_hot[:, trainer : trainer + valider]\n",
        "    y_test = one_hot[:, trainer + valider :]\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98eI17MW4nCT"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(inputs, labels)\n",
        "\n",
        "X_train = X_train.to(device)\n",
        "X_val = X_val.to(device)\n",
        "X_test = X_test.to(device)\n",
        "\n",
        "y_train = y_train.to(device)\n",
        "y_val = y_val.to(device)\n",
        "y_test = y_test.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ2XENXK5MPF"
      },
      "outputs": [],
      "source": [
        "r4net = Neural_Network((768, 120, 2), 0.1, 1)\n",
        "r5net = Neural_Network((768, 50, 2), 0.1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5qsogf-WGPJ"
      },
      "outputs": [],
      "source": [
        "def compare(inputs, labels):\n",
        "    a_net = Neural_Network((768, 120, 2), 0.1, 1)\n",
        "    b_net = Neural_Network((768, 50, 2), 0.1, 1)\n",
        "    for i in range(10):\n",
        "        a_net = Neural_Network((768, 120, 2), 0.1, 1)\n",
        "        b_net = Neural_Network((768, 50, 2), 0.1, 1)\n",
        "        perm = torch.randperm(len(inputs[0]))\n",
        "\n",
        "        shuffled_data = inputs[:, perm]\n",
        "\n",
        "        shuffled_labels = labels[perm]\n",
        "\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test = split_data(shuffled_data, shuffled_labels)\n",
        "\n",
        "        X_train = X_train.to(device)\n",
        "        X_val = X_val.to(device)\n",
        "        X_test = X_test.to(device)\n",
        "\n",
        "        y_train = y_train.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "        y_test = y_test.to(device)\n",
        "        a_net.train_nn(X_train=X_train,\n",
        "               T_train=y_train,\n",
        "               X_valid=X_val,\n",
        "               T_valid=y_val,\n",
        "               alpha=1,\n",
        "               plot=True,\n",
        "               epochs=200)\n",
        "        b_net.train_nn(X_train=X_train,\n",
        "               T_train=y_train,\n",
        "               X_valid=X_val,\n",
        "               T_valid=y_val,\n",
        "               alpha=1,\n",
        "               plot=True,\n",
        "               epochs=200)\n",
        "        print(\"test number:\", i + 1)\n",
        "        print(\"nn A f1 score\", a_net.calc_f1(X_test, y_test).item())\n",
        "        print(\"nn B f1 score\", b_net.calc_f1(X_test, y_test).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uonxpgVCWdAq",
        "outputId": "aa4e38ab-24df-47cc-d2e1-709c08543906"
      },
      "outputs": [],
      "source": [
        "compare(inputs, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNLb2ZZVSrG2",
        "outputId": "4cd808f0-c29b-4b8d-b7de-848c44a09d64"
      },
      "outputs": [],
      "source": [
        "r4net.calc_f1(X_test, y_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
